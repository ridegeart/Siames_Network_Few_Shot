# -*- coding: utf-8 -*-
"""dataPug.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YVceP9LFXBDhunJqVKGs64e4LVVgVo-S
"""
def affinetransform(image):
  transform = AffineTransform(translation=(-30,0))
  warp_image = warp(image,transform, mode="wrap")
  return warp_image
    
def anticlockwise_rotation(image):
  angle= random.randint(0,45)
  return rotate(image, angle)

def clockwise_rotation(image):
  angle= random.randint(0,45)
  return rotate(image, -angle)
      
def transform(self,image):
  if random.random() > 0.5:
    image = affinetransform(image)
  if random.random() > 0.5:
    mage = anticlockwise_rotation(image)
  if random.random() > 0.5:
    image = clockwise_rotation(image)

  return image

from ast import mod
import os
import pickle
from tkinter import UNITS

import tqdm
from PIL import Image
import numpy as np
from skimage.transform import rotate, AffineTransform, warp, rescale
import cv2
import random

class DataLoader(object):
    """
    Class for loading data from image files
    """

    def __init__(self, width, height, cells, data_path, output_path):
        """
        Proper width and height for each image.
        """
        self.width = width
        self.height = height
        self.cells = cells
        self.data_path = data_path
        self.output_path = output_path

    def transform(self,image):
      if random.random() > 0.5:
        image = affinetransform(image)
      if random.random() > 0.5:
        image = anticlockwise_rotation(image)
      if random.random() > 0.5:
        image = clockwise_rotation(image)

      return image

    def _open_image(self, path):
        """
        Using the Image library we open the image in the given path. The path must lead to a .jpg file.
        We then resize it to 105x105 like in the paper (the dataset contains 250x250 images.)
        Returns the image as a numpy array.
        """
        image = Image.open(path)
        image = image.convert('L')
        image = image.resize((self.width, self.height))
        data = np.asarray(image)
        data = np.array(data, dtype='float64')
        return data

    def convert_image_to_array(self, person, image_num, data_path, predict=False):
        """
        Given a person, image number and datapath, returns a numpy array which represents the image.
        predict - whether this function is called during training or testing. If called when training, we must reshape
        the images since the given dataset is not in the correct dimensions.
        """
        image_path = os.path.join(data_path, person, f'{image_num}.jpg')
        image_data = self._open_image(image_path)
        if not predict:
            image_data = image_data.reshape(self.width, self.height, self.cells)
        return image_data

    def load(self, set_name):
        """
        Writes into the given output_path the images from the data_path.
        dataset_type = train or test
        """
        file_path = self.data_path
        print(set_name)
        print('Loading dataset...')
        x_first = []
        x_second = []

        for person_name in os.listdir(self.data_path):
          x_first = []
          person_num = len(os.listdir(os.path.join(self.data_path,person_name)))
          for i in range(100):
            if person_num > 100 :
              idx = random.randint(0,int(person_num)-1)
              while f'{str(idx)}.jpg' not in os.listdir(os.path.join(self.data_path,person_name)):
                idx = random.randint(0,int(person_num)-1)
              image = self.convert_image_to_array(person=person_name,image_num=str(idx),data_path=self.data_path)
              x_first.append(image)

            else:
              if i < person_num:
                num = os.listdir(os.path.join(self.data_path,person_name))
                image = self.convert_image_to_array(person=person_name,image_num=num[i].strip('.jpg'),data_path=self.data_path)
                x_first.append(image)
              else:
                idx = random.randint(0,int(person_num)-1)
                while f'{str(idx)}.jpg' not in os.listdir(os.path.join(self.data_path,person_name)):
                  idx = random.randint(0,int(person_num)-1)
                image_path = os.path.join(self.data_path, person_name, f'{str(idx)}.jpg')
                image_data = self._open_image(image_path)
                image_data = self.transform(image_data)
                image_data = np.asarray(image_data)
                image_data = np.array(image_data, dtype='float64')
                first_image_1 = image_data.reshape(self.width, self.height, self.cells)
                x_first.append(first_image_1)   
          x_first = np.stack(x_first, axis=0)
          x_second.append(x_first)
        x_second = np.stack(x_second, axis=0)

        print('Done loading dataset')
        with open(self.output_path, 'wb') as f:
            pickle.dump(x_second, f)

print("Loaded data loader")

import os
import random
import time
# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import cv2
import threading
import tensorflow as tf
import numpy.random as rng
# %matplotlib inline
import matplotlib.pyplot as plt

output_path = 'D:/pug'
train_path = os.path.join(output_path, "peopleDevTrain_color.pkl")  # A path for the train file
test_path = os.path.join(output_path, "peopleDevTest_color.pkl")
with open(train_path,"rb") as f:
  x_train_List = pickle.load(f)
with open(test_path,"rb") as f:
  x_test_List = pickle.load(f)

x_train_List.shape

x_test_List.shape

import sklearn
from sklearn.metrics import classification_report
from sklearn import metrics

class Siamese_Loader:
    """For loading batches and testing tasks to a siamese net"""
    def __init__(self,Xval):
        self.Xval = Xval
        self.n_val,self.n_ex_val,self.w,self.h,self.cells = Xval.shape
    def get_traingPairs(self,batch_size ,prob=0.5):
        """Create batch of n pairs, half same class, half different class"""
        #train , test = tf.split(y_pred, num_or_size_splits=2, axis=0)
        pairs=[np.zeros((batch_size, self.h, self.w,self.cells)) for i in range(2)]
        targets=np.zeros((batch_size,))
        targets[batch_size//2:] = 1
        left = [];right = []
        target = []
        for _ in range(batch_size):
          res = np.random.choice([0,1],p=[1-prob,prob])
          if res==0:
            p1,p2 = tuple(random.randint(0,(int(self.n_val)-1)) for _ in range(2))
            #選取不同類的輸入對
            p1 = self.Xval[p1,res,:,:]
            p2 = self.Xval[p2,res,:,:]
            left.append(p1);right.append(p2)
            target.append(0)
          else:
            p = np.random.choice(range(0, (int(self.n_val)-1)))
            #選取同類的輸入對
            p1,p2 = self.Xval[p,0,:,:],self.Xval[p,1,:,:]
            left.append(p1);right.append(p2)
            target.append(1)
          pairs = [np.array(left),np.array(right)]
        return pairs, np.array(target)

    def make_oneshot_task(self,N):
        """Create pairs of test image, support set for testing N way one-shot learning. """
        categories = rng.choice(self.n_val,size=(N,),replace=False)
        #idx = random.randint(0,N-1)
        true_category = categories[0]
        test_image = np.asarray([self.Xval[true_category,0,:,:]]*N).reshape(N,self.w,self.h,self.cells)
        test_image = np.array(test_image, dtype='float64')
        support_set = self.Xval[categories,1,:,:]
        support_set[0,:,:] = self.Xval[true_category,1]
        support_set = support_set.reshape(N,self.w,self.h,self.cells)
        support_set =np.array(support_set, dtype='float64')
        pairs = [test_image,support_set]
        targets = np.zeros((N,))
        targets[0] = 1
        targets = np.array(targets, dtype='float64')
        return pairs, targets

    def test_oneshot(self,model,N,k,verbose=0):
        """Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks"""
        pass
        n_correct = 0
        if verbose:
          print("Evaluating model on {} unique {} way one-shot learning tasks ...".format(k,N))
        for i in range(k):
            inputs, targets = self.make_oneshot_task(N)
            probs = model.predict(inputs,verbose=0)
            #odx = np.argmin(targets)
            if np.argmax(probs) == 0:
                n_correct+=1
        print(np.array(probs))
        percent_correct = (100.0*(n_correct) / k)
        for i,p in enumerate(probs):
           if p>0.5:
              probs[i]=1
           else:
              probs[i]=0
        
        print(classification_report(np.array(targets, dtype='int'),np.array(probs)))
        print("Accuracy:",sklearn.metrics.accuracy_score(np.array(targets, dtype='int'),np.array(probs)))
        print("Precision:",sklearn.metrics.precision_score(np.array(targets, dtype='int'),np.array(probs)))
        print("Recall:",sklearn.metrics.recall_score(np.array(targets, dtype='int'),np.array(probs)))
        if verbose:
            print("Got an average of {}% {} way one-shot learning accuracy".format(percent_correct,N))
        return percent_correct

import keras
from sklearn.preprocessing import  OneHotEncoder

def l_softmax_loss(y_true, y_pred, m=4, depth=10):
    depth = len(y_true)
    y_true = tf.cast(y_true, tf.int32)
    y_true = K.one_hot(y_true , depth)  # convert to one-hot labels
    cos_t = y_pred
    sin_t = tf.sqrt(1 - K.square(cos_t))
    cos_mt = tf.cos(m * cos_t - m)  # L-Softmax formula
    sin_mt = tf.sin(m * cos_t - m)
    one_hot = tf.cast(y_true, 'float32')
    loss = K.mean(one_hot * (sin_mt * (m / sin_t) + cos_mt - cos_t))
    return loss

def focal_loss(y_true, y_pred ,alpha=0.25,gamma=2.0):
        gamma_tensor = K.ones_like(y_true) * gamma
        alpha_tensor = K.ones_like(y_true) * alpha

        ce_loss = K.binary_crossentropy(y_true, y_pred)
        p_t = K.exp(-ce_loss)
        focal_loss = alpha_tensor * K.pow((1.0 - p_t), gamma_tensor) * ce_loss

        return focal_loss

def contrastive_loss(y_true, y_pred ,margin):
  
  return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))#1
 #return K.mean((1-y_true) * K.square(y_pred) + (y_true) * K.square(K.maximum(margin - y_pred, 0)))#2

def mixed_loss(y_true, y_pred, alpha=0.25, gamma=2.0, margin=30.0, num_classes=20):
    """
    Mixed loss function that combines L-Softmax loss, Focal loss and Contrastive loss
    :param y_true: True labels
    :param y_pred: Predicted labels
    :param alpha: Weight of Focal loss
    :param gamma: Focusing parameter of Focal loss
    :param margin: Margin for Contrastive loss
    :param num_classes: Number of classes
    :return: Total loss
    """
    #num_classes = len(y_true)
    #print(len(y_true),y_true.shape,y_pred.shape)
    # L-Softmax loss
    #ls_loss = tf.reduce_mean(l_softmax_loss(y_true, y_pred, m=margin, depth=num_classes))
    #ls_loss = tf.reduce_mean(l_softmax_loss(y_true, y_pred, m=margin))
    # Focal loss
    f_loss = tf.reduce_mean(focal_loss(y_true, y_pred, alpha, gamma))

    # Contrastive loss
    #c_loss = tf.reduce_mean(contrastive_loss(y_true, y_pred, margin))

    # Combine losses
    total_loss = f_loss

    return total_loss

def euclidean_distance(vects):
 x, y = vects
 return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))

def eucl_dist_output_shape(shapes):
 shape1, shape2 = shapes
 return (shape1[0], 1)

import tensorflow as tf
from sklearn.model_selection import train_test_split
from keras import Input, Sequential, Model
from keras import backend as K
from keras.callbacks import EarlyStopping
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Lambda, BatchNormalization, Activation,Dropout, Subtract,LeakyReLU
from keras.regularizers import l2
from keras.applications.vgg16 import VGG16
from tensorflow.keras.optimizers import Adam,RMSprop,SGD
from keras.applications.xception import Xception
from keras import backend, layers, metrics
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.mobilenet import MobileNet

class SiameseNetwork(object):
    def __init__(self, seed, width, height, cells, loss, metrics, optimizer, dropout_rate):
        """
        Seed - The seed used to initialize the weights
        width, height, cells - used for defining the tensors used for the input images
        loss, metrics, optimizer, dropout_rate - settings used for compiling the siamese model (e.g., 'Accuracy' and 'ADAM)
        """
        K.clear_session()
        self.load_file = None
        self.seed = seed
        self.initialize_seed()
        self.optimizer = optimizer

        # Define the matrices for the input images
        input_shape = (width, height, cells)
        left_input = Input(input_shape)
        right_input = Input(input_shape)

        # Get the CNN architecture as presented in the paper (read the readme for more information)
        model = self._get_architecture(input_shape)
        encoded_l = model(left_input)
        encoded_r = model(right_input)

        # Add a layer to combine the two CNNs
        """
        L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))
        L1_siamese_dist = L1_layer([encoded_l, encoded_r])
        L1_siamese_dist = Dropout(dropout_rate)(L1_siamese_dist)
        """
        L1_siamese_dist = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([encoded_l, encoded_r])

        # An output layer with Sigmoid activation function
        prediction = Dense(1, activation='sigmoid', bias_initializer=self.initialize_bias)(L1_siamese_dist)

        siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)
        self.siamese_net = siamese_net
        self.siamese_net.compile(loss=loss, optimizer=optimizer, metrics=["accuracy"])

    def initialize_seed(self):
        """
        Initialize seed all for environment
        """
        os.environ['PYTHONHASHSEED'] = str(self.seed)
        random.seed(self.seed)
        np.random.seed(self.seed)
        tf.random.set_seed(self.seed)

    def initialize_weights(self, shape, dtype=None):
        """
        Called when initializing the weights of the siamese model, uses the random_normal function of keras to return a
        tensor with a normal distribution of weights.
        """
        return K.random_normal(shape, mean=0.0, stddev=0.01, dtype=dtype, seed=self.seed)
    def initialize_bias(self, shape, dtype=None):
        """
        Called when initializing the biases of the siamese model, uses the random_normal function of keras to return a
        tensor with a normal distribution of weights.
        """
        return K.random_normal(shape, mean=0.5, stddev=0.01, dtype=dtype, seed=self.seed)
    def _get_architecture(self, input_shape):
        """
        Returns a Convolutional Neural Network based on the input shape given of the images. This is the CNN network
        that is used inside the siamese model. Uses parameters from the siamese one shot paper.
        """
        model = Sequential()
        model.add(
            Conv2D(filters=64,
                   kernel_size=(10, 10),
                   input_shape=input_shape,
                   kernel_initializer=self.initialize_weights,
                   kernel_regularizer=l2(2e-4),
                   name='Conv1'
                   ))
        model.add(Activation("relu"))
        model.add(MaxPooling2D())

        model.add(
            Conv2D(filters=128,
                   kernel_size=(7, 7),
                   kernel_initializer=self.initialize_weights,
                   bias_initializer=self.initialize_bias,
                   kernel_regularizer=l2(2e-4),
                   name='Conv2'
                   ))
        model.add(Activation("relu"))
        model.add(MaxPooling2D())

        model.add(
            Conv2D(filters=128,
                   kernel_size=(4, 4),
                   kernel_initializer=self.initialize_weights,
                   bias_initializer=self.initialize_bias,
                   kernel_regularizer=l2(2e-4),
                   name='Conv3'
                   ))
        model.add(Activation("relu"))
        model.add(MaxPooling2D())

        model.add(
            Conv2D(filters=256,
                   kernel_size=(4, 4),
                   kernel_initializer=self.initialize_weights,
                   bias_initializer=self.initialize_bias,
                   kernel_regularizer=l2(2e-4),
                   name='Conv4'
                   ))
        model.add(Activation("relu"))

        model.add(Flatten())
        model.add(
            Dense(4096,
                  activation='softmax',
                  kernel_initializer=self.initialize_weights,
                  kernel_regularizer=l2(1e-3),
                  bias_initializer=self.initialize_bias))
        return model

#from keras.models import load_weights
import csv

siamese = SiameseNetwork(seed=0, width=105, height=105, cells=3, loss=mixed_loss, metrics=['accuracy'],
                             optimizer= SGD(), dropout_rate=0.4)
#siamese_net = load_model('/content/drive/MyDrive/lfwa/lfw2/best_c_76.0.h5', custom_objects={"W_init": W_init ,"b_init": b_init})
#siamese.siamese_net.load_weights('D:/pug/best_f_88.0.h5')

loss_every = 10
batch_size = 1
N = 10
best = 38

for i in range(0,500000):
  loader = Siamese_Loader(x_train_List)
  x_train, y_train = loader.make_oneshot_task(N)

  x_train_0, x_val_0, y_train_0, y_val_0 = train_test_split(x_train[0], y_train,
                                        test_size=0.2,
                                        random_state=0)
  x_train_1, x_val_1, y_train_1, y_val_1 = train_test_split(x_train[1], y_train,
                                        test_size=0.2,
                                        random_state=0)
  x_train_0 = np.array(x_train_0, dtype='float64')
  x_val_0 = np.array(x_val_0, dtype='float64')
  x_train_1 = np.array(x_train_1, dtype='float64')
  x_val_1 = np.array(x_val_1, dtype='float64')
  x_train = [x_train_0, x_train_1]
  x_val = [x_val_0, x_val_1]
  #print(x_train_0.shape,x_train_1.shape,x_val_0.shape,x_val_1.shape,y_train_0.shape, y_train_1.shape,y_val_0.shape,y_val_1.shape)
  if y_train_0[0] != y_train_1[0] and y_val_0[0] != y_val_1[0]:
    raise Exception("y train lists or y validation list do not equal")

  callback = []
  es = EarlyStopping(monitor='loss', min_delta=0.01, patience=5, mode='auto', verbose=1)
  callback.append(es)

  siamese.siamese_net.fit(x_train, y_train_0, batch_size=batch_size, callbacks=callback, verbose=1)

  if i % loss_every == 0:
    """
    vloss = siamese.siamese_net.test_on_batch(x_val, y_val_0)
    print("iteration {}, validation loss : {:.7f},validation acc : {:.7f}".format(i,vloss[0],vloss[1]))
    """
    loader2 = Siamese_Loader(x_test_List)
    val_acc = loader2.test_oneshot(siamese.siamese_net,N,k=550,verbose=True)

    with open('D:/pug/output.csv', 'a+', newline='') as csvfile:
      # 以空白分隔欄位，建立 CSV 檔寫入器
      writer = csv.writer(csvfile, delimiter=' ')
      writer.writerow([i, val_acc])

    if val_acc >= best:
      best = val_acc
      print("saving")
      siamese.siamese_net.save('D:/pug/best_f_{}.h5'.format(val_acc))